# Project README

## Project Overview

This project implements a web crawler and search engine indexer. It consists of a spider for fetching and indexing web pages, and a test program to output the indexed data.


* `app/src/main`: Contains the source code for the spider and indexer.
* `app/src/test`: Contains the source code for the test program.
* `search_index.db` and `search_index.lg`: Database files generated by the JDBM library.
* `spider_result.txt`: Output file generated by the test program.
* `stopwords.txt`: List of stop words used by the indexer.
* `build.gradle.kts`: Gradle build script.

## Prerequisites

* Java Development Kit (JDK) 11 or higher
* Gradle (included as gradlew)

## Building the Project

1.  **Navigate to the project root directory:**

    ```bash
    cd crawler
    ```

2.  **Build the project using Gradle:**

    * **On Linux/macOS:**

        ```bash
        ./gradlew build
        ```

    * **On Windows:**

        ```bash
        gradlew.bat build
        ```

    This command will compile the source code and create the necessary executable files in the `app/build/libs` directory.

## Running the Spider

1.  **Navigate to the project root directory:**

    ```bash
    cd crawler
    ```

2.  **Run the spider using Gradle:**

    * **On Linux/macOS:**

        ```bash
        ./gradlew run
        ```

      Example:

        ```bash
        ./gradlew run
        ```

    * **On Windows:**

        ```bash
        gradlew.bat run"
        ```

    The spider will fetch the specified number of pages (30 pages), index them, and store the data in `search_index.db`.


## Cleaning the Project

To clean the build files and database, run the following command from the project root directory:

* **On Linux/macOS:**

    ```bash
    ./gradlew clean
    ```

* **On Windows:**

    ```bash
    gradlew.bat clean
    ```

This will delete the `build` directory and the `search_index.db` and `search_index.lg` files.

## Notes

* Ensure that you have a stable internet connection while running the spider.
* The generated files will be found in `/crawler/app/`.
* The `spider_result.txt` file will be created or overwritten each time the test program is run.
* The `search_index.db` and `search_index.lg` files will be created or overwritten each time the spider is run.